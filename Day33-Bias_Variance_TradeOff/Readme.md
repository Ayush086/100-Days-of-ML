# Today's learnings
Today I explored a great concept which is very important in Machine Learning, it often helps in improving model's efficiency. 

## Bias Variance Trade Off
It's a fundamental concept which describes the balance between two types of error that a model can make:  
1. Bias - error due to over simplistic assumptions i.e., model was unable to find out the patterns and underfit the data.
2. Variance - error due to sensitive fluctuations in training data. Model is performing well on training data and failing to give good
results on testing data. (i.e., trying to learn the training data).

Model's performance can be categorised into 3 forms:
1. High Bias Low Variance (HBLV) - model will underfit and unable to find important patterns hidden inside training data. High Bias signifies
that model is biased towards a particular feature/solution and low variance signifies that model is unable to explain the variability present in 
training data.
2. Low Bias High Variance (LBHV) - model will overfit and will try to learn output of each row in dataset. Low Bias indicates that instead finding relations between features and 
output it's learning the results which is why it's unable to find the complex patterns and high variance indicates that model is very sensitive to training data means if we test it with
tiny deflections in input data model will unable to answer it properly.
3. Low Bias Low Variance (LBLV) - ideal condition for any ML model. It means that with low bias model will give equal importance to every feature
and every relation established. And Low Variance signifies that model isn't trying to overfit the data and can able to answer unseen data properly.

***NOTE: It's very important to understand this concept. It's very helping and informative to gain deep knowledge of model developed.***


